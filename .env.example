# ─── Ollama Configuration ─────────────────────────────
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=kimi-k2.5:cloud
EMBEDDING_MODEL=llama3.2:3b

# --- FastPath (Reflex) Configuration ---
FAST_PATH_ENABLED=true
FAST_PATH_MODEL=llama3.2:3b
FAST_PATH_HOST=http://localhost:11434
USE_PROXY=false
HTTP_PROXY=http://test1:Tes%4012345@10.20.1.222:3128
HTTPS_PROXY=http://test1:Tes%4012345@10.20.1.222:3128
NO_PROXY=localhost,127.0.0.1,10.20.1.222

# ─── Model Parameters ─────────────────────────────────
OLLAMA_TEMPERATURE=0.3
OLLAMA_CONTEXT_SIZE=32768
OLLAMA_TIMEOUT=120

# ─── Agent Configuration ──────────────────────────────
AGENT_NAME=devops-agent
GCC_BASE_PATH=./.GCC
SKILLS_PATH=./skills
LOG_LEVEL=INFO

# ─── Observability (Langfuse) ──────────────────────────
# Start local stack: docker-compose -f docker-compose.langfuse.yml up -d
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=http://localhost:3000
